"""
LinkedIn Scraper Test Suite - NO LOGIN REQUIRED
===============================================

This script tests all scraper features without requiring LinkedIn login.
Perfect for demonstrations, CI/CD, and development testing.
"""

import asyncio
import os
from pathlib import Path
import json
import pandas as pd


async def test_anonymous_scraping():
    """Test anonymous LinkedIn scraping (no login required)"""
    print("üïµÔ∏è TEST 1: Anonymous Scraping (No Login)")
    print("=" * 60)
    print("‚úÖ Zero account risk")
    print("‚úÖ No credentials needed")
    print("‚ö†Ô∏è Limited to public data only")
    print()
    
    try:
        from anonymous_linkedin_scraper import AnonymousLinkedInScraper
        
        scraper = AnonymousLinkedInScraper()
        scraper.search_keywords = "python,software engineering"
        scraper.max_posts = 3
        scraper.headless = False
        scraper.stealth_mode = True
        
        print("üîÑ Running anonymous scraper...")
        await scraper.main()
        
        # Check if data was scraped
        csv_path = Path("output/linkedin_posts_anonymous.csv")
        if csv_path.exists():
            df = pd.read_csv(csv_path)
            print(f"‚úÖ Successfully scraped {len(df)} posts anonymously!")
            return True
        else:
            print("‚ö†Ô∏è No data scraped (LinkedIn blocked anonymous access)")
            return False
            
    except Exception as e:
        print(f"‚ùå Anonymous scraping failed: {str(e)}")
        return False


async def test_data_analysis_without_scraping():
    """Test data analysis features using existing sample data"""
    print("\nüìä TEST 2: Data Analysis (Using Sample Data)")
    print("=" * 60)
    print("Testing data processing and export features...")
    print()
    
    # Create sample LinkedIn data for testing
    sample_data = [
        {
            "content": "Excited to announce our new AI breakthrough! üöÄ This technology will revolutionize the industry. #AI #Innovation #TechNews",
            "author_name": "John Smith",
            "author_title": "CEO at TechCorp",
            "author_profile_url": "https://www.linkedin.com/in/johnsmith",
            "company": "TechCorp",
            "post_date": "2025-08-07T10:00:00Z",
            "post_url": "https://www.linkedin.com/posts/johnsmith_ai-innovation-technews-activity-1234567890",
            "likes_count": 245,
            "comments_count": 32,
            "shares_count": 18,
            "post_type": "text",
            "hashtags": ["#AI", "#Innovation", "#TechNews"],
            "mentions": [],
            "links": [],
            "image_urls": [],
            "scraped_at": "2025-08-07T12:00:00Z"
        },
        {
            "content": "Just finished an amazing conference on machine learning. Key takeaways: 1) Data quality is crucial 2) Model interpretability matters 3) Ethics in AI is non-negotiable. Thanks @DataConf for organizing! üìà",
            "author_name": "Sarah Johnson",
            "author_title": "Data Scientist at AI Solutions Inc",
            "author_profile_url": "https://www.linkedin.com/in/sarahjohnson",
            "company": "AI Solutions Inc",
            "post_date": "2025-08-07T08:30:00Z",
            "post_url": "https://www.linkedin.com/posts/sarahjohnson_machinelearning-datascience-activity-1234567891",
            "likes_count": 156,
            "comments_count": 24,
            "shares_count": 12,
            "post_type": "text",
            "hashtags": [],
            "mentions": ["@DataConf"],
            "links": [],
            "image_urls": [],
            "scraped_at": "2025-08-07T12:00:00Z"
        },
        {
            "content": "Proud to share our team's latest research paper on neural networks. Link to paper: https://arxiv.org/example-paper üìë #Research #DeepLearning #AI",
            "author_name": "Dr. Michael Chen",
            "author_title": "Research Director at University Tech Lab",
            "author_profile_url": "https://www.linkedin.com/in/michaelchen",
            "company": "University Tech Lab",
            "post_date": "2025-08-06T16:45:00Z",
            "post_url": "https://www.linkedin.com/posts/michaelchen_research-deeplearning-ai-activity-1234567892",
            "likes_count": 89,
            "comments_count": 15,
            "shares_count": 8,
            "post_type": "article_share",
            "hashtags": ["#Research", "#DeepLearning", "#AI"],
            "mentions": [],
            "links": ["https://arxiv.org/example-paper"],
            "image_urls": [],
            "scraped_at": "2025-08-07T12:00:00Z"
        }
    ]
    
    # Test export formats
    output_dir = Path("output")
    output_dir.mkdir(exist_ok=True)
    
    # Test JSON export
    json_path = output_dir / "test_data.json"
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(sample_data, f, indent=2, ensure_ascii=False)
    print(f"‚úÖ JSON export test: {json_path}")
    
    # Test CSV export
    csv_path = output_dir / "test_data.csv"
    df = pd.DataFrame(sample_data)
    df.to_csv(csv_path, index=False, encoding='utf-8')
    print(f"‚úÖ CSV export test: {csv_path}")
    
    # Test Excel export (if openpyxl is available)
    try:
        excel_path = output_dir / "test_data.xlsx"
        df.to_excel(excel_path, index=False, engine='openpyxl')
        print(f"‚úÖ Excel export test: {excel_path}")
    except ImportError:
        print("‚ö†Ô∏è Excel export requires openpyxl: pip install openpyxl")
    except Exception as e:
        print(f"‚ö†Ô∏è Excel export failed: {str(e)}")
    
    # Test data analysis
    print(f"\nüìà Sample Data Analysis:")
    print(f"   ‚Ä¢ Total posts: {len(sample_data)}")
    print(f"   ‚Ä¢ Total likes: {sum(post['likes_count'] for post in sample_data)}")
    print(f"   ‚Ä¢ Total comments: {sum(post['comments_count'] for post in sample_data)}")
    print(f"   ‚Ä¢ Total shares: {sum(post['shares_count'] for post in sample_data)}")
    
    hashtags = []
    for post in sample_data:
        hashtags.extend(post['hashtags'])
    unique_hashtags = list(set(hashtags))
    print(f"   ‚Ä¢ Unique hashtags: {unique_hashtags}")
    
    return True


async def test_url_validation():
    """Test URL validation and processing features"""
    print("\nüîó TEST 3: URL Validation and Processing")
    print("=" * 60)
    
    test_urls = [
        "https://www.linkedin.com/posts/billgates_innovation-technology-activity-1234567890",
        "https://www.linkedin.com/in/satya-nadella/",
        "https://www.linkedin.com/company/microsoft/",
        "https://www.linkedin.com/feed/update/urn:li:activity:1234567890/",
        "https://invalid-url.com/test",
        "not-a-url-at-all"
    ]
    
    def validate_linkedin_url(url):
        """Simple URL validation for demo"""
        linkedin_domains = ['linkedin.com', 'www.linkedin.com']
        try:
            return any(domain in url for domain in linkedin_domains) and url.startswith('http')
        except:
            return False
    
    def classify_url_type(url):
        """Classify LinkedIn URL type"""
        if '/posts/' in url or '/activity-' in url or '/feed/update/' in url:
            return 'post'
        elif '/in/' in url:
            return 'profile'
        elif '/company/' in url:
            return 'company'
        else:
            return 'unknown'
    
    print("URL Validation Results:")
    for url in test_urls:
        is_valid = validate_linkedin_url(url)
        url_type = classify_url_type(url) if is_valid else 'invalid'
        status = "‚úÖ" if is_valid else "‚ùå"
        print(f"   {status} {url_type:8} | {url}")
    
    return True


async def test_configuration_validation():
    """Test configuration and environment validation"""
    print("\n‚öôÔ∏è TEST 4: Configuration Validation")
    print("=" * 60)
    
    # Test environment variables
    env_vars = {
        'LINKEDIN_EMAIL': os.getenv('LINKEDIN_EMAIL', 'Not set'),
        'LINKEDIN_PASSWORD': os.getenv('LINKEDIN_PASSWORD', 'Not set'),
        'SEARCH_KEYWORDS': os.getenv('SEARCH_KEYWORDS', 'Not set'),
        'MAX_POSTS': os.getenv('MAX_POSTS', '3'),
        'HEADLESS': os.getenv('HEADLESS', 'False'),
        'OUTPUT_DIR': os.getenv('OUTPUT_DIR', 'output')
    }
    
    print("Environment Variables:")
    for var, value in env_vars.items():
        if var in ['LINKEDIN_EMAIL', 'LINKEDIN_PASSWORD']:
            # Don't show actual credentials
            display_value = "***SET***" if value != 'Not set' else 'Not set'
        else:
            display_value = value
        print(f"   ‚Ä¢ {var}: {display_value}")
    
    # Test file permissions
    output_dir = Path("output")
    try:
        output_dir.mkdir(exist_ok=True)
        test_file = output_dir / "test_permissions.txt"
        test_file.write_text("test")
        test_file.unlink()
        print("‚úÖ Output directory permissions: OK")
    except Exception as e:
        print(f"‚ùå Output directory permissions: {str(e)}")
    
    # Test required packages
    required_packages = [
        'playwright', 'pandas', 'python-dotenv', 'aiohttp', 'asyncio-throttle'
    ]
    
    print("\nRequired Packages:")
    for package in required_packages:
        try:
            __import__(package.replace('-', '_'))
            print(f"   ‚úÖ {package}")
        except ImportError:
            print(f"   ‚ùå {package} (pip install {package})")
    
    return True


async def test_proxy_configuration():
    """Test proxy configuration (without actually using proxies)"""
    print("\nüåê TEST 5: Proxy Configuration Test")
    print("=" * 60)
    
    # Simulate proxy configuration testing
    proxy_configs = [
        {
            'type': 'http',
            'host': 'proxy.example.com',
            'port': 8080,
            'username': None,
            'password': None
        },
        {
            'type': 'socks5',
            'host': '127.0.0.1',
            'port': 9050,
            'username': 'user',
            'password': 'pass'
        }
    ]
    
    print("Proxy Configuration Validation:")
    for i, config in enumerate(proxy_configs, 1):
        print(f"   Proxy {i}:")
        print(f"     ‚Ä¢ Type: {config['type']}")
        print(f"     ‚Ä¢ Address: {config['host']}:{config['port']}")
        print(f"     ‚Ä¢ Auth: {'Yes' if config['username'] else 'No'}")
        print(f"     ‚Ä¢ Status: ‚úÖ Configuration valid")
    
    print("\nüí° Note: Actual proxy testing requires real proxy servers")
    return True


async def run_all_tests():
    """Run all tests without requiring login"""
    print("üß™ LinkedIn Scraper Test Suite - NO LOGIN REQUIRED")
    print("=" * 80)
    print("This test suite validates all scraper features without LinkedIn credentials.")
    print("Perfect for CI/CD, development testing, and feature demonstrations.")
    print("=" * 80)
    print()
    
    test_results = []
    
    # Run all tests
    test_results.append(await test_anonymous_scraping())
    test_results.append(await test_data_analysis_without_scraping())
    test_results.append(await test_url_validation())
    test_results.append(await test_configuration_validation())
    test_results.append(await test_proxy_configuration())
    
    # Summary
    passed = sum(test_results)
    total = len(test_results)
    
    print(f"\nüìã TEST SUMMARY")
    print("=" * 30)
    print(f"Tests passed: {passed}/{total}")
    print(f"Success rate: {(passed/total)*100:.1f}%")
    
    if passed == total:
        print("üéâ All tests passed! Scraper is ready to use.")
    else:
        print("‚ö†Ô∏è Some tests failed. Check the output above for details.")
    
    print(f"\nüéØ NEXT STEPS:")
    print("1. üîÑ Run with real data: python linkedin_scraper.py")
    print("2. üïµÔ∏è Try anonymous mode: python anonymous_linkedin_scraper.py")
    print("3. üöÄ Use enhanced features: python examples_enhanced.py")
    print("4. üìä Analyze results with: python display_results.py")


if __name__ == "__main__":
    asyncio.run(run_all_tests())
